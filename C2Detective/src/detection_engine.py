import json
import sqlite3
from colorama import Fore
from scapy.all import *
import tldextract
from ipaddress import ip_address
import itertools
import re
from urllib.parse import urlparse
import ml_model

"""
start_time :                                timestamp when packet capture stared :  string :        %Y-%m-%d %H:%M:%S
end_time :                                  timestamp when packet capture ended :   string :        %Y-%m-%d %H:%M:%S
connection_frequency :                      grouped TCP connections frequencies :   {} :            {(src_ip, src_port, dst_ip, dst_port):count, ...} 
external_tcp_connections :                  all TCP connections :                   [] :            [ (packet_time, src_ip, src_port, dst_ip, dst_port), ... ]                  
public_src_ip_list/_dst_ip_list/_ip_list :  all public source/destination IPs :     [] :            [ ip, ip, ... ] 
src_/dst_/combined_/unique_ip_list :        unique source/destination IPs :         [] :            [ ip, ip, ... ]
src_ip_/dst_ip_/all_ip_/counter :           IP quantity :                           {} :            { ip:count, ip:count, ... }
dns_packets :                               extracted packets with DNS layer :      [] :            [packet, packet, ...]
domain_names :                              extracted domain names from DNS :        list() :        [ domain, domain, ... ]
http_payloads :                             HTTP payloads :                         [] :            [ payload, payload, ... ]
http_sessions :                             HTTP sessions :                         [{}, {}, ...] : [ {time: ,src_ip:, src_port:, dst_ip:, dst_port:, http_payload:}, {}, ... ]  
unique_urls :                               extracted URLs :                        list() :        [ url, url, ... ]
connections :                               gruped connections :                    tuple :         ( (PROTOCOL SRC_IP:SRC_PORT > DST_IP:DST_PORT), ... )
certificates :                              selected TLS certificate fields :       [] :            [ {src_ip, dst_ip, src_port, dst_port, serialNumber, issuer:{organizationName, stateOrProvinceName, countryName, commonName}, subject:{} }, ...]
"""



def get_domain_whitelist(domain_whitelist_path):
    whitelisted_domains = []

    print(f"[{time.strftime('%H:%M:%S')}] [INFO] Loading whitelisted domain names ...")
    logging.info("Loading whitelisted domain names")

    with open(domain_whitelist_path, "r") as whitelist:
        whitelisted_domains = whitelist.read().splitlines()

    return whitelisted_domains


# Looking for connections with excessive frequency
def print_dga_domains(detected_domains):
    print(
        f"[{time.strftime('%H:%M:%S')}] [INFO] Listing detected domain names generated by Domain Generation Algorithms (DGA)")
    logging.info(
        f"Listing detected domain names generated by Domain Generation Algorithms (DGA)")
    for domain in detected_domains:
        print(f">> {Fore.RED}{domain}{Fore.RESET}")


def print_dns_tunneling_indicators(detected_queries):
    print(f"[{time.strftime('%H:%M:%S')}] [INFO] Listing information about detected DNS Tunneling technique")
    logging.info(
        f"Listing information about detected DNS Tunneling technique")

    for domain, data in detected_queries.items():
        print(
            f">> Queried {len(data['queries'])} unique subdomains for '{Fore.RED}{domain}{Fore.RESET}'")

def print_dns_tunneling_indicators_model(detected_queries):
    print(f"[{time.strftime('%H:%M:%S')}] [INFO] Listing information about detected DNS Tunneling technique")
    logging.info(
        f"Listing information about detected DNS Tunneling technique")

    for data in detected_queries:
        print(
            f">> Queried from SOURCE: {data['src']} with Query: '{Fore.RED}{data['query']}{Fore.RESET}'")



def print_c2_ip_addresses(detected_ips):
    print(
        f"[{time.strftime('%H:%M:%S')}] [INFO] Listing detected C2 IP addresses")
    logging.info(f"Listing detected C2 IP addresses")

    for c2_ip_address in detected_ips:
        print(f">> {Fore.RED}{c2_ip_address}{Fore.RESET}")


def print_c2_domains(detected_domains):
    print(
        f"[{time.strftime('%H:%M:%S')}] [INFO] Listing detected domain names for C2 servers")
    logging.info(f"Listing detected domain names for C2 servers")
    for domain in detected_domains:
        print(f">> {Fore.RED}{domain}{Fore.RESET}")


def print_c2_urls(detected_urls):
    print(
        f"[{time.strftime('%H:%M:%S')}] [INFO] Listing detected C2 related URLs")
    logging.info(f"Listing detected C2 related URLs")
    for url in detected_urls:
        print(f">> {Fore.RED}{url}{Fore.RESET}")


class DetectionEngine:
    def __init__(self, c2_indicators_total_count, analyst_profile, packet_parser):
        self.logger = logging.getLogger(__name__)
        self.c2_indicators_total_count = c2_indicators_total_count
        self.c2_indicators_count = 0
        self.c2_indicators_detected = False
        self.detected_iocs = {}
        self.detected_iocs['aggregated_ip_addresses'] = set()
        self.detected_iocs['aggregated_domain_names'] = set()
        self.detected_iocs['aggregated_urls'] = set()

        self.packet_parser = packet_parser

        self.CHUNK_SIZE = analyst_profile.chunk_size
        self.MAX_FREQUENCY = len(
            self.packet_parser.packets) * (analyst_profile.MAX_FREQUENCY / 100)
        self.MAX_DURATION = analyst_profile.MAX_DURATION
        self.MAX_HTTP_SIZE = analyst_profile.MAX_HTTP_SIZE
        self.MAX_SUBDOMAIN_LENGTH = analyst_profile.MAX_SUBDOMAIN_LENGTH

        self.whitelisted_domains = get_domain_whitelist(
            analyst_profile.domain_whitelist_path)
        self.detected_iocs['filepath'] = self.packet_parser.get_filepath()

    def evaluate_detection(self):
        if self.c2_indicators_detected:
            if self.c2_indicators_count < self.c2_indicators_total_count / 2:
                print(
                    f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.YELLOW}Potential Command & Control communication indicators were detected{Fore.RESET}")
            else:
                print(
                    f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.RED}Potential Command & Control communication indicators were detected{Fore.RESET}")
            logging.info(
                f"Command & Control communication indicators detected")
            print(
                f">> Number of detected indicators: {self.c2_indicators_count}/{self.c2_indicators_total_count}")
        else:
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.GREEN}Command & Control communication indicators not detected{Fore.RESET}")
            logging.info(
                f"Command & Control communication indicators not detected")

    # ----------------------------------------------------------------------------------------------------------------
    # ------------------------------------------- NETWORK TRAFFIC DETECTION ------------------------------------------
    # ----------------------------------------------------------------------------------------------------------------

    def detect_connections_with_excessive_frequency(self):
        print(
            f"[{time.strftime('%H:%M:%S')}] [INFO] Looking for connections with excessive frequency ...")
        logging.info("Looking for connections with excessive frequency")

        detected = False
        detected_connections = []

        # find connections with excessive frequency
        for connection, count in self.packet_parser.connection_frequency.items():
            if count > self.MAX_FREQUENCY:
                detected = True

                src_ip = connection[0]
                src_port = connection[1]
                dst_ip = connection[2]
                dst_port = connection[3]

                entry = dict(
                    src_ip=src_ip,
                    src_port=src_port,
                    dst_ip=dst_ip,
                    dst_port=dst_port,
                    frequency=count
                )
                # print(f"Connection {connection} has {count} packets, which is over {threshold:.0f}% of total packets.")
                detected_connections.append(entry)
                self.detected_iocs['aggregated_ip_addresses'].add(src_ip)
                self.detected_iocs['aggregated_ip_addresses'].add(dst_ip)

        if detected:
            self.c2_indicators_detected = True
            self.c2_indicators_count += 1
            self.detected_iocs['excessive_frequency'] = detected_connections
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.RED}Detected connections with excessive frequency{Fore.RESET}")
            logging.info(
                f"Detected connections with excessive frequency. (detected_connections : {detected_connections})")
            self.print_connections_with_excessive_frequency(
                detected_connections)
        else:
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.GREEN}Connections with excessive frequency not detected{Fore.RESET}")
            logging.info(f"Connections with excessive frequency not detected")

    def print_connections_with_excessive_frequency(self, detected_connections):
        print(
            f"[{time.strftime('%H:%M:%S')}] [INFO] Listing connections with excessive frequency")
        logging.info(f"Listing connections with excessive frequency")

        for entry in detected_connections:
            print(
                f">> {Fore.RED}{entry.get('src_ip')}:{entry.get('src_port')} -> {entry.get('dst_ip')}:{entry.get('dst_port')}{Fore.RESET} = {entry.get('frequency')}/{len(self.packet_parser.external_tcp_connections)} connections")

    def detect_dga(self):
        # source : https://lindevs.com/disable-tensorflow-2-debugging-information
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # suppress TensorFlow logging
        from dgad.prediction import Detective

        print(
            f"[{time.strftime('%H:%M:%S')}] [INFO] Hunting domain names generated by Domain Generation Algorithms (DGA) ...")
        logging.info(
            "Hunting domain names generated by Domain Generation Algorithms (DGA)")
        dga_detected = False
        detected_domains = []

        detective = Detective()
        # convert extracted domain names strings into dgad.schema.Domain
        converted_domains, _ = detective.prepare_domains(
            self.packet_parser.domain_names)
        # classify them
        detective.investigate(converted_domains)

        for entry in converted_domains:
            report = str(entry)
            if "is_dga=True" in report:
                raw_split = report.split("raw='")[1]
                dga_domain = raw_split.split("', words=")[0]
                dga_detected = True
                detected_domains.append(dga_domain)
                self.detected_iocs['aggregated_domain_names'].add(dga_domain)

        if dga_detected:
            self.c2_indicators_detected = True
            self.c2_indicators_count += 1
            self.detected_iocs['DGA_domains'] = detected_domains
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.RED}Detected domain names generated by Domain Generation Algorithms (DGA){Fore.RESET}")
            logging.info(
                f"Detected domain names generated by Domain Generation Algorithms (DGA). (detected_domains : {detected_domains})")
            print_dga_domains(detected_domains)
        else:
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.GREEN}Domains generated by Domain Generation Algorithms (DGA) not detected{Fore.RESET}")
            logging.info(
                f"Domains generated by Domain Generation Algorithms (DGA) not detected")

    def detect_dns_tunneling(self):
        print(
            f"[{time.strftime('%H:%M:%S')}] [INFO] Looking for indicators of Long Subdomain ...")
        logging.info("Looking for indicators of Long Subdomain")

        detected = False
        detected_queries = {}

        for packet in self.packet_parser.dns_packets:
            if packet.haslayer(DNSQR):  # pkt.qr == 0 ; DNS query
                query = packet[DNSQR].qname.decode('utf-8', errors='ignore')
                subdomain, domain, suffix = tldextract.extract(query)

                if "arpa" in suffix:  # provides namespaces for reverse DNS lookups
                    continue
                if "gov.in" in suffix:
                    continue
                if "gov" in suffix:
                    continue

                if self.whitelisted_domains:  # user defined list whitelisted domain names
                    detected_whitelisted_domain = False
                    for w_domain in self.whitelisted_domains:
                        _, w_domain_name, _ = tldextract.extract(w_domain)
                        if domain == w_domain_name:
                            # continue the outermost for loop
                            detected_whitelisted_domain = True

                if detected_whitelisted_domain:
                    continue

                if len(subdomain) > self.MAX_SUBDOMAIN_LENGTH:  # check for long domain names
                    detected = True
                    domain = f"{domain}.{suffix}"  # rebuild domain with TLD

                    if domain in detected_queries:
                        detected_queries[domain]['queries'].add(query)
                    else:
                        detected_queries[domain] = {'queries': {query}}

        for domain in detected_queries:
            queries = list(detected_queries[domain]['queries'])
            detected_queries[domain]['queries'] = queries
            self.detected_iocs['aggregated_domain_names'].update(queries)
        # print("------------------>",self.detected_iocs,"<------------------")

        if detected:
            self.c2_indicators_detected = True
            self.c2_indicators_count += 1
            self.detected_iocs['DNS_Tunneling'] = detected_queries
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.RED}Detected Queries with Long Subdomain{Fore.RESET}")
            logging.info(
                f"Long Subdomain queries detected. (detected_queries : {detected_queries})")
            print_dns_tunneling_indicators(detected_queries)
        else:
            print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.GREEN}Long Subdomain not detected{Fore.RESET}")
            logging.info(f"Long Subdomain not detected")

    def detect_from_ml_model(self,input_file):
        print(
            f"[{time.strftime('%H:%M:%S')}] [INFO] Looking for indicators of DNS Tunneling technique Using ML Model ...")
        logging.info("Looking for indicators of DNS Tunneling using ML Model")
        recent_df = ml_model.start(input_file)
        filtered_df = recent_df[recent_df['prediction'] == 1].drop_duplicates(subset=['query'])
        detected_model = False
        self.detected_iocs["DNS_Tunneling_Model"] = {}
        # print(filtered_df)
        if not filtered_df.empty:
          detected_model = True
          self.c2_indicators_detected = True
          self.c2_indicators_count += 1
          filtered_dict = filtered_df.to_dict(orient='records')
          self.detected_iocs["DNS_Tunneling_Model"] = filtered_dict
          print(
                f"[{time.strftime('%H:%M:%S')}] [INFO]{Fore.RED}Detected DNS Tunneling from Model{Fore.RESET}")
          logging.info(
                f"Detected DNS Tunneling technique from Model. (detected_queries : {filtered_dict})")
          print_dns_tunneling_indicators_model(filtered_dict)
        else:
          print(
                f"[{time.strftime('%H:%M:%S')}] [INFO] {Fore.GREEN}DNS Tunneling not detected by ML Model{Fore.RESET}")
          logging.info(f"DNS Tunneling technique not detected by ML Model")
          
        print("model executed succesfully")
        recent_df = recent_df.dropna()
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        # Define the file handler for the log file
        log_file_path = 'dataframe_log.log'
        file_handler = logging.FileHandler(log_file_path,mode='a')
        file_handler.setLevel(logging.INFO)

        # Define the formatter
        formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        file_handler.setFormatter(formatter)

        # Add the file handler to the logger
        logger.addHandler(file_handler)

        # Convert the dataframe to a string and log it
        # recent_df = recent_df[recent_df["pred_is_dns_data_exfiltration"]!=]
        df_str = recent_df.to_string(index=False)
        logger.info(f"File:{input_file} , DataFrame:\n%s", df_str)

        print("DataFrame stored in log file:", log_file_path)

    

    
    # ----------------------------------------------------------------------------------------------------------------

    def get_detected_iocs(self):
        self.logger.info(
            f"Preparing detected IoCs for writing to the output file")
        self.detected_iocs['aggregated_ip_addresses'] = list(
            self.detected_iocs['aggregated_ip_addresses'])
        self.detected_iocs['aggregated_domain_names'] = list(
            self.detected_iocs['aggregated_domain_names'])
        self.detected_iocs['aggregated_urls'] = list(
            self.detected_iocs['aggregated_urls'])

        ip_list = self.detected_iocs['aggregated_ip_addresses']
        public_ips = [ip for ip in ip_list if not ip_address(ip).is_private]
        self.detected_iocs['aggregated_ip_addresses'] = public_ips

        return self.detected_iocs


    def get_c2_indicators_count(self):
        return self.c2_indicators_count
